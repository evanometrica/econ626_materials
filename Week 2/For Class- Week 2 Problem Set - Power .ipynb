{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(seed=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this homework assignment, we are going to use simulations to calculate statistical power and learn about statistical power.  We are going to use the experiment run by AirBnB using simultaneous reveal of reviews as loose inspiration. We will focus on the effect that simultaneous reveal had on the probability of a host leaving a review. \n",
    "\n",
    "Ultimately, we want to find the smallest sample size that AirBnB would have needed to prove that offering simultaneous review increases the probability that the host would leave a review. In this assignment, you should also learn some intuition about how your requirements for statistical power change as some parameters of your experiment change. It is common for analysts in industry to test many different assumptions and designs when calculating power\n",
    "\n",
    "In **Question 1**, you will need to:\n",
    "- create simulated *random* samples for treatment and control, based on the parameters reported in the paper. \n",
    "- Use the proportion t-test from the statsmodels package to calculate whether or not we can reject the null hypothesis that treatment and control have the same proportion of hosts writing reviews\n",
    "- Loop through this many times to see how often we reject the null hypothesis, when drawing different samples with the same proportions. \n",
    "\n",
    "\n",
    "In **Question 2**,  you are going to try some alternative tools besides simulations to calculate power, besides the simulation. These are tools that you can rely on if you ever have to do a power calculation on your own. \n",
    "- You will try the power calculator from statsmodels.stats.power to calculate power and minimum sample size\n",
    "- You can try the analytic formula provided  here to calculate the minimum sample size needed to maintain 80% power. \n",
    "\n",
    "In **Question 3**, you are going to return to the simulation method you built up in question 1. \n",
    "- You are going to learn how power changes as function of sample size \n",
    "- You are going to see how you should allocate users between treatment and control to maximize power\n",
    "- You are going to learn how to calculate “minimum detectable effect size” for a fixed population size\n",
    "\n",
    "These types of calculations would be very similar to what you might do as an analyst or data scientist if you were planning a follow-up experiment to the paper described in the paper we read. You might take the results from that experiment as a starting point, and build on top of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Simulating Power\n",
    "## 45 points\n",
    "\n",
    "In this question, we are going to calculate the statistical power of an experiment similar to the AirBnB simultaneous reveal experiment. However, we will begin by looking at what would have happened if they had a much smaller sample size, of only 100 users in each of the treatment and control cells\n",
    "\n",
    "**A (5 points)**: Use the random methods in numpy to create a random list of 1's and 0, representing if a host gave a review or not, focusing on the control cell. \n",
    "We will use the data from the paper and assume the user left a review 72% of the time\n",
    "We will assume a sample size of 100 in each  \n",
    "Provide an array of length 100 of 1 and 0s with a 72% chance of a 1. Call this “control_sample”\n",
    "\n",
    "\n",
    "**B (5 points)**: What is the # of times from your list they left a review \n",
    "- *Hint: it probably won't be exactly 72, because you are creating a random variable, with an expected value of 72, but any given draw will vary. If you want to check your work, try repeating this many times, and check the mean  is approximately 72%* \n",
    "\n",
    "**C (5 points)** : You're going to need to do these types of random draws many many times in this homework. So, turn what you did in parts A & B into a function.  Create a function named countReviews. This function should have two arguments:\n",
    "- p = probability that a host left a review\n",
    "- n = sample size \n",
    "This function should return the number of users from each sample that left a review. \n",
    "\n",
    "**D (5 points)** : Use the function created in part C to simulate the treatment group, which had a probability of 79% that the host left a review. Keep your sample size at 100. What is the number of times a host left of a review in your simulated treatment cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E (5 points)**: Use the proportions_ztest function calculate the z-stat and the p-value. Using a two-sided test, test whether two means (from parts B and D) have the same proportion of host leaving a review. Can you reject the null hypothesis that they do, at 95% confidence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F (10 points)**: Now you are going to simulate what you did in part E 1000 times. \n",
    "- Make an empty list to keep track of your results\n",
    "- Make a loop that will repeat 1000 times\n",
    "\n",
    "Then, inside the loop\n",
    "-  Use the function you created in C twice, once to draw a  control sample (72% chance of leaving a review ) and once to draw the treatment sample (79% chance of leaving a review). Both samples can have 100 hosts\n",
    "- Use the proportion z test just like you did in part E. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**G (5 points)** : In what percent of simulations from part F did you reject the null hypothesis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**H (5 points)**: How does the percentage in part G relate to statistical power ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Calculating Power in Other Ways\n",
    "## (25 points)\n",
    "\n",
    "Now that you practiced calculating power on your own with a simulation, let’s practice using some other tools that can also help\n",
    "\n",
    "**A (10 points)** : Using *statsmodels.stats.power.zt_ind_solve_power* calculate the power of experiment using the same parameters used in part A. In particular we want:\n",
    "    - 95% confidence on a two-sided test\n",
    "    - 72% of hosts leaving reviews in treatment, 79% in control\n",
    "    - Assume a sample size of 100 in each cell\n",
    "\n",
    "*Hint, you should get a similar (but probably not exactly the same, within ~3 percentage points) in part 2.A and part 1.G* \n",
    "\n",
    "**B (5 points)**: As you can see, we do not have 80% power from the experiment as designed. Use *zt_ind_solve_power* to calculate how big a sample size you would need for 80% power (5 points)\n",
    "\n",
    "**C 10 points)**: There is also a formula you can use to calculate the sample size you need. Make sure the required sample size you calculated in 2B is similar to what you calculate with this formula\n",
    "-  n = (Zα+Zβ)^2 * (p1(1-p1)+p2(1-p2)) / (p1-p2)^2\n",
    "- For a two-sided t-test, you can use 1.96 for your critical for 95% confidence. you can use .84 as your critical value for 80% power\n",
    "\n",
    "*Hint, B and C will not exactly match. This is because we calculate the variance in slightly different ways for each calculation. We discussed in class why. You should see a difference of ~5 users per group*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import zt_ind_solve_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C  n = (Zα+Zβ)^2 * (p1(1-p1)+p2(1-p2)) / (p1-p2)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note**\n",
    "You may be wondering why we did simulations in Part 1 and will return to them in Part 3, if we have nice functions to do the work for us. First of all, I hope the simulations give you some intuition on how to do the homework. Secondarily, a lot of times, you will find that your method of analysis doesn’t exactly match a simple t-test like above. For example, in Week 4, we will talk about using linear regressions for your analysis. It is very very common to need to build simulations to do power analyses of relatively complex experiments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Testing Some Assumptions\n",
    "## 30 points \n",
    "**A (there are no points for this question. Doing this will make B,C,D easier)**: Take what you built to answer question 1G and wrap it in a function. This means you should be running 1000 simulations, drawing from samples, and calculating whether you reject the null hypotheses in each simulation. \n",
    "\n",
    "You should have 4 arguments in this function, called “calc_power”. If should have 4 arguments \n",
    "- p_c:  Probability of host leaving a review when in control group\n",
    "- p_t:  Probability of host leaving a review when in treatment group\n",
    "- n_c = sample size for control\n",
    "- n_t = sample size for treatment\n",
    "\n",
    "This function should return the % of simulations where you reject the null hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B (10 points**):  See what happens as your sample sizes increases.  I’ve created a list of sample sizes below (n_obs). You should take this as your total sample size. So n_treatment and n_control should equal each value of n_obs divided by 2.  In part 2B and 2C, when we calculated sample size it was for one cell, but it is more common to know your entire addressable population and need to distribute it amongst cells. \n",
    "- Loop through sample sizes in n_obs. Capture power from each sample size\n",
    "- Put n_obs and the output of your loop into a dataframe. Plot n_obs on the x-axis and power on the y-axis\n",
    "- At what value of n_obs is power approximately equal to 80% (hint, this should be about double what you calculated in 2b and 2c). At what value is it approximately equal to 0.95\n",
    "\n",
    "*Hint: if you get an error, it might be because np.random.choice is expecting an integer for the ‘size’ argument (buried all the way back in 1.C) Try putting an ‘int()’ around whatever calculations you do to create ‘n_c’, and ‘n_t’*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = [100, 200, 300, 400, 800, 1000, 1200, 1500, 2000,5000]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C (10 points)**:  Now, examine what happens if you do not allocate equally to treatment and control. Assume we have a sample size of 2000 people total.  Simulate what would happen if 10% were in control and 90% were in treatment, 20% control/80% treatment, 30% control/70% treatment, etc. \n",
    "\n",
    "I have provided a line of code below that makes a list of percent of population allocated to control called \"percents_control\". \n",
    "\n",
    "- Use the function you created in part 3.A to calculate power with different splits between treatment controls.\n",
    "- Loop through percent_control  where  n_control should equal 2000*p and n_treatment should equal 2000*(1-p). \n",
    "- After you loop through thi, put both lists into a dataframe. Plot percent_treatment on the x-axis and power on the y axis. At what split between treatment and control is power maximized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "percents_control= np.linspace(.1,.9,9)\n",
    "percents_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D (10 points)** Now we want to learn for a fixed sample size, what the smallest effect size you will be powered to detect (at 95 percent confidence and 80% power).This is commonly referred to as the minimal detectable effect size. This is a common calculation when you know how many units you can potentially treat, and want to see if you would be powered to observe a result\n",
    "- in this case we are going to keep our sample size at 2000 (n_c= 1000 and n_t=1000). We will keep the control treatment the same at .72. \n",
    "- Loop through different p_t’s from the  \"ps_treatment\" array provided below.\n",
    "- What is the smallest p_t that produces more than 80% power? \n",
    "- What is the the minimum detectable effect size is for a population of 2000 (it should be value you just calculated - 0.72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.725, 0.73 , 0.735, 0.74 , 0.745, 0.75 , 0.755, 0.76 , 0.765,\n",
       "       0.77 , 0.775, 0.78 , 0.785, 0.79 , 0.795, 0.8  ])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps_treatment = np.arange(.725, .8,.005)\n",
    "ps_treatment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
